{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3eb4d-d9a1-47c5-b881-c380c2e7d3cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ì…€ 1: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (UI) ì„¤ì •\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Google Colabì˜ displayì™€ ipywidgetsì˜ displayê°€ ì¶©ëŒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª…ì‹œì ìœ¼ë¡œ ìŠ¤íƒ€ì¼ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "display(HTML(\"<style>.widget-label { min-width: 25ex !important; }</style>\"))\n",
    "\n",
    "class UI:\n",
    "    def __init__(self):\n",
    "        # --- ëª¨ë¸ ì„¤ì • ---\n",
    "        self.model_quant = widgets.Dropdown(options=[\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\"], value=\"Q4_K_M\", description=\"Model Quantization:\")\n",
    "        self.lightx2v_rank = widgets.Dropdown(options=[\"32\", \"64\", \"128\"], value=\"32\", description=\"Lightx2v Rank:\")\n",
    "\n",
    "        # --- LoRA ë‹¤ìš´ë¡œë“œ ì„¤ì • ---\n",
    "        self.download_lora_1 = widgets.Checkbox(value=False, description=\"Download LoRA 1\")\n",
    "        self.lora_1_url = widgets.Text(value=\"Put your LoRA URL here\", description=\"LoRA 1 URL:\")\n",
    "        self.download_lora_2 = widgets.Checkbox(value=False, description=\"Download LoRA 2\")\n",
    "        self.lora_2_url = widgets.Text(value=\"Put your LoRA URL here\", description=\"LoRA 2 URL:\")\n",
    "        self.download_lora_3 = widgets.Checkbox(value=False, description=\"Download LoRA 3\")\n",
    "        self.lora_3_url = widgets.Text(value=\"https://huggingface.co/Remade-AI/Rotate/resolve/main/rotate_20_epochs.safetensors\", description=\"LoRA 3 URL:\")\n",
    "        self.civitai_token = widgets.Password(description=\"Civitai Token:\", value=\"\")\n",
    "\n",
    "        # --- ë¹„ë””ì˜¤ ìƒì„± ì„¤ì • ---\n",
    "        self.positive_prompt = widgets.Textarea(value=\"slow jumping and fast dancing\", description=\"Positive Prompt:\", layout={'width': '95%', 'height': '80px'})\n",
    "        self.prompt_assist = widgets.Dropdown(options=[\"none\", \"walking to camera\", \"walking from camera\", \"swaying\"], value=\"none\", description=\"Prompt Assist:\")\n",
    "        self.negative_prompt = widgets.Textarea(value=\"è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°\", description=\"Negative Prompt:\", layout={'width': '95%', 'height': '80px'})\n",
    "        self.width = widgets.IntText(value=720, description=\"Width:\")\n",
    "        self.height = widgets.IntText(value=1280, description=\"Height:\")\n",
    "        self.seed = widgets.IntText(value=0, description=\"Seed (0=random):\")\n",
    "        self.high_noise_steps = widgets.IntSlider(value=3, min=1, max=25, step=1, description=\"High Noise Steps:\")\n",
    "        self.steps = widgets.IntSlider(value=6, min=1, max=50, step=1, description=\"Total Steps:\")\n",
    "        self.cfg_scale = widgets.FloatSlider(value=1.0, min=1.0, max=20.0, step=0.1, description=\"CFG Scale:\")\n",
    "        self.sampler_name = widgets.Dropdown(options=[\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"], value=\"euler\", description=\"Sampler:\")\n",
    "        self.scheduler = widgets.Dropdown(options=[\"simple\",\"normal\",\"karras\",\"exponential\",\"sgm_uniform\",\"ddim_uniform\",\"beta\",\"linear_quadratic\",\"kl_optimal\"], value=\"simple\", description=\"Scheduler:\")\n",
    "        self.frames = widgets.IntSlider(value=81, min=1, max=120, step=1, description=\"Frames:\")\n",
    "        self.overwrite_previous_video = widgets.Checkbox(value=True, description=\"Overwrite Previous Video\")\n",
    "\n",
    "        # --- ëª¨ë¸ ê³ ê¸‰ ì„¤ì • ---\n",
    "        self.use_sage_attention = widgets.Checkbox(value=True, description=\"Use Sage Attention:\")\n",
    "        self.use_flow_shift = widgets.Checkbox(value=True, description=\"Use Flow Shift:\")\n",
    "        self.flow_shift = widgets.FloatSlider(value=8.0, min=0.0, max=100.0, step=0.01, description=\"Flow Shift 1:\")\n",
    "        self.flow_shift2 = widgets.FloatSlider(value=8.0, min=0.0, max=100.0, step=0.01, description=\"Flow Shift 2:\")\n",
    "\n",
    "        # --- Wan 2.1 LoRA ì„¤ì • ---\n",
    "        self.use_lightx2v = widgets.Checkbox(value=True, description=\"Use Lightx2v:\")\n",
    "        self.lightx2v_Strength = widgets.FloatSlider(value=3.0, min=-10.0, max=10.0, step=0.01, description=\"Lightx2v Strength:\")\n",
    "        self.use_lightx2v2 = widgets.Checkbox(value=True, description=\"Use Lightx2v2 (Pusa):\")\n",
    "        self.lightx2v2_Strength = widgets.FloatSlider(value=1.5, min=-10.0, max=10.0, step=0.01, description=\"Lightx2v2 Strength:\")\n",
    "\n",
    "        # --- ì»¤ìŠ¤í…€ LoRA ì„¤ì • ---\n",
    "        self.use_lora_1 = widgets.Checkbox(value=False, description=\"Use Custom LoRA 1:\")\n",
    "        self.lora_1_strength = widgets.FloatSlider(value=1.0, min=-10.0, max=10.0, step=0.01, description=\"LoRA 1 Strength:\")\n",
    "        self.use_lora_2 = widgets.Checkbox(value=False, description=\"Use Custom LoRA 2:\")\n",
    "        self.lora_2_strength = widgets.FloatSlider(value=1.0, min=-10.0, max=10.0, step=0.01, description=\"LoRA 2 Strength:\")\n",
    "        self.use_lora_3 = widgets.Checkbox(value=False, description=\"Use Custom LoRA 3:\")\n",
    "        self.lora_3_strength = widgets.FloatSlider(value=1.0, min=-10.0, max=10.0, step=0.01, description=\"LoRA 3 Strength:\")\n",
    "\n",
    "        # --- Teacache ì„¤ì • ---\n",
    "        self.rel_l1_thresh = widgets.FloatSlider(value=0, min=0.0, max=10.0, step=0.001, description=\"Rel L1 Thresh:\")\n",
    "        self.start_percent = widgets.FloatSlider(value=0.2, min=0.0, max=1.0, step=0.01, description=\"Start Percent:\")\n",
    "        self.end_percent = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.01, description=\"End Percent:\")\n",
    "        \n",
    "        # --- í”„ë ˆì„ ë³´ê°„ ì„¤ì • ---\n",
    "        self.interpolate_video = widgets.Checkbox(value=True, description=\"Apply Frame Interpolation\")\n",
    "        self.frame_multiplier = widgets.IntText(value=2, description=\"Frame Multiplier:\")\n",
    "        self.interpolated_fps = widgets.IntText(value=30, description=\"Interpolated FPS:\")\n",
    "        self.crf_value = widgets.IntSlider(value=17, min=0, max=51, step=1, description=\"CRF (Quality):\")\n",
    "        \n",
    "        # --- íŒŒì¼ ì—…ë¡œë“œ ---\n",
    "        self.image_uploader = widgets.FileUpload(accept='image/*', description='Upload Image')\n",
    "        self.display_upload_check = widgets.Checkbox(value=False, description=\"Display uploaded image\")\n",
    "\n",
    "\n",
    "    def display_ui(self):\n",
    "        lora_downloads = widgets.VBox([\n",
    "            self.download_lora_1, self.lora_1_url,\n",
    "            self.download_lora_2, self.lora_2_url,\n",
    "            self.download_lora_3, self.lora_3_url,\n",
    "            self.civitai_token\n",
    "        ])\n",
    "        \n",
    "        generation_settings = widgets.VBox([\n",
    "            self.positive_prompt, self.prompt_assist, self.negative_prompt,\n",
    "            widgets.HBox([self.width, self.height]),\n",
    "            self.seed, self.high_noise_steps, self.steps, self.cfg_scale,\n",
    "            self.sampler_name, self.scheduler, self.frames, self.overwrite_previous_video\n",
    "        ])\n",
    "        \n",
    "        model_config = widgets.VBox([\n",
    "            self.use_sage_attention,\n",
    "            widgets.HBox([self.use_flow_shift, self.flow_shift, self.flow_shift2])\n",
    "        ])\n",
    "\n",
    "        wan_lora_config = widgets.VBox([\n",
    "            widgets.HBox([self.use_lightx2v, self.lightx2v_Strength]),\n",
    "            widgets.HBox([self.use_lightx2v2, self.lightx2v2_Strength])\n",
    "        ])\n",
    "        \n",
    "        custom_lora_config = widgets.VBox([\n",
    "            widgets.HBox([self.use_lora_1, self.lora_1_strength]),\n",
    "            widgets.HBox([self.use_lora_2, self.lora_2_strength]),\n",
    "            widgets.HBox([self.use_lora_3, self.lora_3_strength])\n",
    "        ])\n",
    "\n",
    "        teacache_settings = widgets.VBox([\n",
    "            self.rel_l1_thresh, self.start_percent, self.end_percent\n",
    "        ])\n",
    "        \n",
    "        interpolation_settings = widgets.VBox([\n",
    "            self.interpolate_video, self.frame_multiplier, self.interpolated_fps, self.crf_value\n",
    "        ])\n",
    "\n",
    "        accordion = widgets.Accordion(children=[\n",
    "            generation_settings,\n",
    "            widgets.VBox([self.model_quant, self.lightx2v_rank]),\n",
    "            lora_downloads,\n",
    "            model_config,\n",
    "            wan_lora_config,\n",
    "            custom_lora_config,\n",
    "            teacache_settings,\n",
    "            interpolation_settings\n",
    "        ])\n",
    "        \n",
    "        accordion.set_title(0, 'ğŸ“ Video Settings')\n",
    "        accordion.set_title(1, 'ğŸ¤– Base Model Settings')\n",
    "        accordion.set_title(2, 'ğŸ“¥ LoRA Downloads')\n",
    "        accordion.set_title(3, 'âš™ï¸ Model Advanced Config')\n",
    "        accordion.set_title(4, 'ğŸš€ Wan 2.1 LoRA Config')\n",
    "        accordion.set_title(5, 'ğŸ¨ Custom LoRA Config')\n",
    "        accordion.set_title(6, 'ğŸ§  Teacache Settings')\n",
    "        accordion.set_title(7, 'âœ¨ Frame Interpolation')\n",
    "\n",
    "        upload_box = widgets.VBox([self.image_uploader, self.display_upload_check])\n",
    "        \n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h1>ğŸ’¥ ComfyUI Video Generation Settings</h1>\"),\n",
    "            widgets.HTML(\"<h3>1. íŒŒì¼ ì—…ë¡œë“œ</h3>\"),\n",
    "            upload_box,\n",
    "            widgets.HTML(\"<hr><h3>2. ìƒì„± ì˜µì…˜</h3>\"),\n",
    "            accordion\n",
    "        ]))\n",
    "\n",
    "# UI ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í‘œì‹œ\n",
    "ui = UI()\n",
    "ui.display_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d3c7d-bacf-492c-b95a-7fc6465aeaf2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ì…€ 2: ì‚¬ì „ì„¤ì •\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output, display, HTML, Image as IPImage\n",
    "import random\n",
    "\n",
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "print(\"Installing required libraries...\")\n",
    "!pip install torch==2.6.0 torchvision==0.21.0\n",
    "!pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2 triton==3.2.0 sageattention\n",
    "!pip install -q av spandrel albumentations insightface onnx opencv-python segment_anything ultralytics onnxruntime onnxruntime-gpu\n",
    "!apt -y install -qq aria2 ffmpeg\n",
    "clear_output(wait=True)\n",
    "print(\"Library installation complete.\")\n",
    "\n",
    "# 2. ComfyUI ë° ì»¤ìŠ¤í…€ ë…¸ë“œ í´ë¡  ë° ì„¤ì •\n",
    "print(\"Setting up ComfyUI and custom nodes...\")\n",
    "if not os.path.exists('/content/ComfyUI'):\n",
    "    !git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\n",
    "    %cd /content/ComfyUI/custom_nodes\n",
    "    !git clone https://github.com/city96/ComfyUI-GGUF ComfyUI_GGUF\n",
    "    !git clone https://github.com/kijai/ComfyUI-KJNodes ComfyUI_KJNodes\n",
    "    %cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
    "    !pip install -r requirements.txt\n",
    "    %cd /content/ComfyUI/custom_nodes/ComfyUI_KJNodes\n",
    "    !sed -i 's/^/#/' /content/ComfyUI/custom_nodes/ComfyUI_KJNodes/__init__.py\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "# 3. RIFE ëª¨ë¸ ì„¤ì¹˜\n",
    "if not os.path.exists('/content/Practical-RIFE'):\n",
    "    %cd /content\n",
    "    !git clone https://github.com/Isi-dev/Practical-RIFE\n",
    "    %cd /content/Practical-RIFE\n",
    "    !pip install git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation\n",
    "    os.makedirs('/content/Practical-RIFE/train_log', exist_ok=True)\n",
    "    !wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/IFNet_HDv3.py -O /content/Practical-RIFE/train_log/IFNet_HDv3.py\n",
    "    !wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/RIFE_HDv3.py -O /content/Practical-RIFE/train_log/RIFE_HDv3.py\n",
    "    !wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/refine.py -O /content/Practical-RIFE/train_log/refine.py\n",
    "    !wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/flownet.pkl -O /content/Practical-RIFE/train_log/flownet.pkl\n",
    "\n",
    "%cd /content/ComfyUI\n",
    "!sed -i -e 's/^from server import PromptServer/# from server import PromptServer/' -e '/^\\\\s*if unique_id:/s/^/    # /' -e '/PromptServer\\\\.instance\\\\.send_progress_text/s/^/            # /' /content/ComfyUI/comfy_extras/nodes_images.py\n",
    "clear_output(wait=True)\n",
    "print(\"ComfyUI setup complete.\")\n",
    "\n",
    "# 4. íŒŒì´ì¬ í™˜ê²½ ì„¤ì • ë° ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "sys.path.insert(0, '/content/ComfyUI')\n",
    "\n",
    "from comfy import model_management\n",
    "from nodes import CheckpointLoaderSimple, CLIPLoader, CLIPTextEncode, VAEDecode, VAELoader, KSampler, KSamplerAdvanced, UNETLoader, LoadImage, SaveImage, CLIPVisionLoader, CLIPVisionEncode, LoraLoaderModelOnly, ImageScale\n",
    "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
    "from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import WanVideoTeaCacheKJ, PathchSageAttentionKJ, WanVideoNAG, SkipLayerGuidanceWanVideo\n",
    "from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
    "from comfy_extras.nodes_images import SaveAnimatedWEBP\n",
    "from comfy_extras.nodes_video import SaveWEBM\n",
    "from comfy_extras.nodes_wan import WanImageToVideo\n",
    "from comfy_extras.nodes_upscale_model import UpscaleModelLoader\n",
    "\n",
    "# --- START OF ALL HELPER FUNCTIONS ---\n",
    "# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ (ìˆ˜ì •ë¨)\n",
    "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
    "    import os\n",
    "    filename = link.split(\"/\")[-1]\n",
    "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
    "    print(\"Executing download command:\")\n",
    "    print(command)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    get_ipython().system(command) # os.system ëŒ€ì‹  ì‚¬ìš©\n",
    "    return filename\n",
    "\n",
    "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
    "    import os\n",
    "    import time\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    try:\n",
    "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
    "    except IndexError:\n",
    "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
    "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
    "    if civitai_token:\n",
    "        civitai_url += f\"&token={civitai_token}\"\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"model_{timestamp}.safetensors\"\n",
    "    full_path = os.path.join(folder, filename)\n",
    "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
    "    print(\"Downloading from Civitai...\")\n",
    "    get_ipython().system(download_command) # os.system ëŒ€ì‹  ì‚¬ìš©\n",
    "    local_path = os.path.join(folder, filename)\n",
    "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
    "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
    "    else:\n",
    "        print(f\"âŒ LoRA download failed or file is empty: {local_path}\")\n",
    "    return filename\n",
    "\n",
    "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
    "    if \"civitai.com\" in link.lower():\n",
    "        if not civitai_token:\n",
    "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
    "        return download_civitai_model(link, civitai_token, folder)\n",
    "    else:\n",
    "        return download_with_aria2c(link, folder)\n",
    "\n",
    "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
    "    try:\n",
    "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
    "        if filename is None:\n",
    "            filename = url.split('/')[-1].split('?')[0]\n",
    "        cmd = ['aria2c', '--console-log-level=error', '-c', '-x', '16', '-s', '16', '-k', '1M', '-d', dest_dir, '-o', filename, url]\n",
    "        if silent:\n",
    "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
    "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
    "        subprocess.run(cmd, check=True, capture_output=silent, text=True) # subprocessëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "        if silent:\n",
    "            print(\"Done!\")\n",
    "        else:\n",
    "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
    "        return filename\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\nError downloading {filename}: {e.stderr.strip() if e.stderr else 'Unknown error'}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def image_width_height(image):\n",
    "    if image.ndim == 4: _, height, width, _ = image.shape\n",
    "    elif image.ndim == 3: height, width, _ = image.shape\n",
    "    else: raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
    "    return width, height\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
    "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
    "    with imageio.get_writer(output_path, fps=fps) as writer:\n",
    "        for frame in frames: writer.append_data(frame)\n",
    "    return output_path\n",
    "\n",
    "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
    "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
    "    Image.fromarray(frame).save(output_path)\n",
    "    return output_path\n",
    "\n",
    "def save_as_webm(images, filename_prefix, fps, codec=\"vp9\", quality=32, output_dir=\"/content/ComfyUI/output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True); output_path = f\"{output_dir}/{filename_prefix}.webm\"\n",
    "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
    "    kwargs = {'fps': int(fps), 'quality': int(quality), 'codec': str(codec), 'output_params': ['-crf', str(int(quality))]}\n",
    "    with imageio.get_writer(output_path, format='FFMPEG', mode='I', **kwargs) as writer:\n",
    "        for frame in frames: writer.append_data(frame)\n",
    "    return output_path\n",
    "\n",
    "def display_video(video_path):\n",
    "    from base64 import b64encode\n",
    "    video_data = open(video_path,'rb').read()\n",
    "    mime_type = f\"video/{video_path.split('.')[-1]}\"\n",
    "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
    "    display(HTML(f'<video width=512 controls autoplay loop><source src=\"{data_url}\" type=\"{mime_type}\"></video>'))\n",
    "\n",
    "# --- ëˆ„ë½ë˜ì—ˆë˜ swapT í•¨ìˆ˜ ì¶”ê°€ ---\n",
    "def swapT(pa, f, s):\n",
    "    return s if pa == f else pa\n",
    "\n",
    "output_path = \"\"\n",
    "output_pathU = \"\"\n",
    "\n",
    "# ======================================================================\n",
    "# ========= THE MAIN GENERATE_VIDEO FUNCTION STARTS HERE ===============\n",
    "# ======================================================================\n",
    "def generate_video(\n",
    "    image_path: str = None,\n",
    "    LoRA_Strength: float = 1.00,\n",
    "    rel_l1_thresh: float = 0.275,\n",
    "    start_percent: float = 0.1,\n",
    "    end_percent: float = 1.0,\n",
    "    positive_prompt: str = \"a cute anime girl with massive fennec ears and a big fluffy tail wearing a maid outfit turning around\",\n",
    "    prompt_assist: str = \"walking to viewers\",\n",
    "    negative_prompt: str = \"...\",\n",
    "    width: int = 832,\n",
    "    height: int = 480,\n",
    "    seed: int = 82628696717253,\n",
    "    steps: int = 20,\n",
    "    cfg_scale: float = 1.0,\n",
    "    sampler_name: str = \"uni_pc\",\n",
    "    scheduler: str = \"simple\",\n",
    "    frames: int = 33,\n",
    "    fps: int = 16,\n",
    "    output_format: str = \"mp4\",\n",
    "    overwrite: bool = False,\n",
    "    use_lora: bool = True,\n",
    "    use_lora2: bool = True,\n",
    "    LoRA_Strength2: float = 1.00,\n",
    "    use_lora3: bool = True,\n",
    "    LoRA_Strength3: float = 1.00,\n",
    "    use_lightx2v: bool = False,\n",
    "    lightx2v_Strength: float = 0.80,\n",
    "    lightx2v_steps: int = 4,\n",
    "    use_pusa: bool = False,\n",
    "    pusa_Strength: float = 1.2,\n",
    "    pusa_steps: int = 6,\n",
    "    use_sage_attention: bool = True,\n",
    "    enable_flow_shift: bool = True,\n",
    "    shift: float = 8.0,\n",
    "    enable_flow_shift2: bool = True,\n",
    "    shift2: float = 8.0,\n",
    "    end_step1: int = 10,\n",
    "):\n",
    "    with torch.inference_mode():\n",
    "        unet_loader = UnetLoaderGGUF(); pathch_sage_attention = PathchSageAttentionKJ(); wan_video_nag = WanVideoNAG(); teacache = WanVideoTeaCacheKJ(); model_sampling = ModelSamplingSD3(); clip_loader = CLIPLoader(); clip_encode_positive = CLIPTextEncode(); clip_encode_negative = CLIPTextEncode(); vae_loader = VAELoader(); clip_vision_loader = CLIPVisionLoader(); clip_vision_encode = CLIPVisionEncode(); load_image = LoadImage(); wan_image_to_video = WanImageToVideo(); ksampler = KSamplerAdvanced(); vae_decode = VAEDecode(); save_webp = SaveAnimatedWEBP(); save_webm = SaveWEBM(); pAssLora = LoraLoaderModelOnly(); load_lora = LoraLoaderModelOnly(); load_lora2 = LoraLoaderModelOnly(); load_lora3 = LoraLoaderModelOnly(); load_lightx2v_lora = LoraLoaderModelOnly(); load_pusa_lora = LoraLoaderModelOnly(); image_scaler = ImageScale()\n",
    "        print(\"Loading Text_Encoder...\"); clip = clip_loader.load_clip(\"umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"wan\", \"default\")[0]; positive = clip_encode_positive.encode(clip, positive_prompt)[0]; negative = clip_encode_negative.encode(clip, negative_prompt)[0]; del clip; torch.cuda.empty_cache(); gc.collect()\n",
    "        if image_path is None: print(\"No image uploaded!\"); return\n",
    "        loaded_image = load_image.load_image(image_path)[0]; width_int, height_int = image_width_height(loaded_image)\n",
    "        if height == 0: height = int(width * height_int / width_int)\n",
    "        print(f\"Image resolution is {width_int}x{height_int}\"); print(f\"Scaling image to {width}x{height}...\")\n",
    "        loaded_image = image_scaler.upscale(loaded_image, \"lanczos\", width, height, \"disabled\")[0]\n",
    "        clip_vision_output = None\n",
    "        print(\"Loading VAE...\"); vae = vae_loader.load_vae(\"wan_2.1_vae.safetensors\")[0]\n",
    "        positive_out, negative_out, latent = wan_image_to_video.encode(positive, negative, vae, width, height, frames, 1, loaded_image, clip_vision_output)\n",
    "        usedSteps = steps\n",
    "        print(\"Loading high noise Model...\"); model = unet_loader.load_unet(dit_model)[0]\n",
    "        if enable_flow_shift: model = model_sampling.patch(model, shift)[0]\n",
    "        if prompt_assist != \"none\":\n",
    "            if prompt_assist == \"walking to viewers\": print(\"Loading walking to camera LoRA...\"); model = pAssLora.load_lora_model_only(model, walkingToViewersL, 1)[0]\n",
    "            if prompt_assist == \"walking from behind\": print(\"Loading walking from camera LoRA...\"); model = pAssLora.load_lora_model_only(model, walkingFromBehindL, 1)[0]\n",
    "            if prompt_assist == \"b3ll13-d8nc3r\": print(\"Loading dancing LoRA...\"); model = pAssLora.load_lora_model_only(model, dancingL, 1)[0]\n",
    "        if use_lora and lora_1 is not None: print(\"Loading LoRA...\"); model = load_lora.load_lora_model_only(model, lora_1, LoRA_Strength)[0]\n",
    "        if use_lora2 and lora_2 is not None: print(\"Loading LoRA 2...\"); model = load_lora2.load_lora_model_only(model, lora_2, LoRA_Strength2)[0]\n",
    "        if use_lora3 and lora_3 is not None: print(\"Loading LoRA 3...\"); model = load_lora3.load_lora_model_only(model, lora_3, LoRA_Strength3)[0]\n",
    "        if use_lightx2v: print(\"Loading lightx2v LoRA...\"); model = load_lightx2v_lora.load_lora_model_only(model, lightx2v_lora, lightx2v_Strength)[0]; usedSteps=lightx2v_steps\n",
    "        if use_sage_attention: model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
    "        if rel_l1_thresh > 0: print(\"Setting Teacache...\"); model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"14B\")[0]\n",
    "        clear_output()\n",
    "        print(\"Generating video with high noise model...\"); sampled = ksampler.sample(model=model, add_noise=\"enable\", noise_seed=seed, steps=usedSteps, cfg=cfg_scale, sampler_name=sampler_name, scheduler=scheduler, positive=positive_out, negative=negative_out, latent_image=latent, start_at_step=0, end_at_step=end_step1, return_with_leftover_noise=\"enable\")[0]\n",
    "        del model; torch.cuda.empty_cache(); gc.collect()\n",
    "        print(\"Loading low noise Model...\"); model = unet_loader.load_unet(dit_model2)[0]\n",
    "        if enable_flow_shift2: model = model_sampling.patch(model, shift2)[0]\n",
    "        if prompt_assist != \"none\":\n",
    "            if prompt_assist == \"walking to viewers\": print(\"Loading walking to camera LoRA...\"); model = pAssLora.load_lora_model_only(model, walkingToViewersL, 1)[0]\n",
    "            if prompt_assist == \"walking from behind\": print(\"Loading walking from camera LoRA...\"); model = pAssLora.load_lora_model_only(model, walkingFromBehindL, 1)[0]\n",
    "            if prompt_assist == \"b3ll13-d8nc3r\": print(\"Loading dancing LoRA...\"); model = pAssLora.load_lora_model_only(model, dancingL, 1)[0]\n",
    "        if use_lora and lora_1 is not None: print(\"Loading LoRA...\"); model = load_lora.load_lora_model_only(model, lora_1, LoRA_Strength)[0]\n",
    "        if use_lora2 and lora_2 is not None: print(\"Loading LoRA 2...\"); model = load_lora2.load_lora_model_only(model, lora_2, LoRA_Strength2)[0]\n",
    "        if use_lora3 and lora_3 is not None: print(\"Loading LoRA 3...\"); model = load_lora3.load_lora_model_only(model, lora_3, LoRA_Strength3)[0]\n",
    "        if use_pusa: print(\"Loading pusav1 LoRA...\"); model = load_pusa_lora.load_lora_model_only(model, lightx2v_lora, pusa_Strength)[0]; usedSteps=pusa_steps\n",
    "        if use_sage_attention: model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
    "        if rel_l1_thresh > 0: print(\"Setting Teacache...\"); model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"14B\")[0]\n",
    "        clear_output()\n",
    "        print(\"Generating video with low noise model...\"); sampled = ksampler.sample(model=model, add_noise=\"disable\", noise_seed=seed, steps=usedSteps, cfg=cfg_scale, sampler_name=sampler_name, scheduler=scheduler, positive=positive_out, negative=negative_out, latent_image=sampled, start_at_step=end_step1, end_at_step=10000, return_with_leftover_noise=\"disable\")[0]\n",
    "        del model; torch.cuda.empty_cache(); gc.collect()\n",
    "        try:\n",
    "            print(\"Decoding latents...\"); decoded = vae_decode.decode(vae, sampled)[0]; del vae; torch.cuda.empty_cache(); gc.collect()\n",
    "            global output_path; import datetime; base_name = \"ComfyUI\"\n",
    "            if not overwrite: base_name += f\"_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            if frames == 1: print(\"Single frame, saving as PNG...\"); output_path = save_as_image(decoded[0], \"ComfyUI\"); display(IPImage(filename=output_path))\n",
    "            else:\n",
    "                if output_format.lower() == \"webm\": print(\"Saving as WEBM...\"); output_path = save_as_webm(decoded, base_name, fps=fps, codec=\"vp9\", quality=10)\n",
    "                elif output_format.lower() == \"mp4\": print(\"Saving as MP4...\"); output_path = save_as_mp4(decoded, base_name, fps)\n",
    "                else: raise ValueError(f\"Unsupported format: {output_format}\")\n",
    "                display_video(output_path)\n",
    "        except Exception as e: print(f\"Error during decoding/saving: {str(e)}\"); raise\n",
    "        finally: clear_memory()\n",
    "\n",
    "# --- END OF ALL HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "# 6. UI ì„¤ì •ì— ë”°ë¼ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì´ ë¶€ë¶„ì€ UI ê°ì²´ 'ui'ê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)\n",
    "# ì•„ë˜ ì½”ë“œëŠ” 'ui'ë¼ëŠ” ê°ì²´ê°€ ì´ì „ì— ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "# ë§Œì•½ 'ui' ê°ì²´ê°€ ì—†ë‹¤ë©´, ì´ ë¶€ë¶„ì€ ì‹¤í–‰ë˜ì§€ ì•Šê±°ë‚˜ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "try:\n",
    "    print(\"Downloading models based on UI settings...\")\n",
    "\n",
    "    # LoRA ë‹¤ìš´ë¡œë“œ\n",
    "    lora_1, lora_2, lora_3 = None, None, None\n",
    "    if ui.download_lora_1.value:\n",
    "        lora_1 = download_lora(ui.lora_1_url.value, civitai_token=ui.civitai_token.value)\n",
    "    if ui.download_lora_2.value:\n",
    "        lora_2 = download_lora(ui.lora_2_url.value, civitai_token=ui.civitai_token.value)\n",
    "    if ui.download_lora_3.value:\n",
    "        lora_3 = download_lora(ui.lora_3_url.value, civitai_token=ui.civitai_token.value)\n",
    "    clear_output(wait=True)\n",
    "    print(\"LoRA download process finished.\")\n",
    "\n",
    "    # ë©”ì¸ ëª¨ë¸ (Dit) ë‹¤ìš´ë¡œë“œ\n",
    "    model_quant_val = ui.model_quant.value\n",
    "    if model_quant_val == \"Q4_K_M\":\n",
    "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "        dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "    elif model_quant_val == \"Q5_K_M\":\n",
    "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "        dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "    elif model_quant_val == \"Q6_K\":\n",
    "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "        dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "    else: # Q8_0\n",
    "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "        dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "\n",
    "    # í•„ìˆ˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
    "    model_download(\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"/content/ComfyUI/models/text_encoders\")\n",
    "    model_download(\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors\", \"/content/ComfyUI/models/vae\")\n",
    "    model_download(\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors\", \"/content/ComfyUI/models/clip_vision\")\n",
    "\n",
    "    # Lightx2v LoRA ë‹¤ìš´ë¡œë“œ\n",
    "    lightx2v_rank_val = ui.lightx2v_rank.value\n",
    "    if lightx2v_rank_val == \"32\":\n",
    "        lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_I2V_14B_480p_cfg_step_distill_rank32_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "    elif lightx2v_rank_val == \"64\":\n",
    "        lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank64_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "    else: # 128\n",
    "        lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank128_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "\n",
    "    # ë³´ì¡° LoRA ë‹¤ìš´ë¡œë“œ\n",
    "    walkingToViewersL = model_download(\"https://huggingface.co/Isi99999/Wan2.1_14B-480p_I2V_LoRAs/resolve/main/walking%20to%20viewers_Wan.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "    walkingFromBehindL = model_download(\"https://huggingface.co/Isi99999/Wan2.1_14B-480p_I2V_LoRAs/resolve/main/walking_from_behind.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "    dancingL = model_download(\"https://huggingface.co/Isi99999/Wan2.1_14B-480p_I2V_LoRAs/resolve/main/b3ll13-d8nc3r.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"âœ… Environment Setup and Model Download Complete!\")\n",
    "    file_uploaded = None # ì—…ë¡œë“œëœ íŒŒì¼ ê²½ë¡œë¥¼ ì €ì¥í•  ë³€ìˆ˜\n",
    "\n",
    "except NameError:\n",
    "    print(\"UI object 'ui' is not defined. Skipping UI-based model downloads.\")\n",
    "    print(\"Please define the 'ui' object or download models manually if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3559ac9-a648-49da-8248-e507faa79f65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ì…€ 3: ì´ë¯¸ì§€ ì—…ë¡œë“œ (ë‘ ë²ˆì§¸ ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „)\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# UI ìœ„ì ¯ì—ì„œ ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "uploaded_file_info = ui.image_uploader.value\n",
    "\n",
    "if not uploaded_file_info:\n",
    "    print(\"â€¼ï¸ ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš” (ì…€ 1ì˜ 'Upload Image' ë²„íŠ¼ ì‚¬ìš©).\")\n",
    "else:\n",
    "    # ë§ˆì§€ë§‰(ì²« ë²ˆì§¸)ìœ¼ë¡œ ì—…ë¡œë“œëœ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    last_uploaded_file = uploaded_file_info[0]\n",
    "    \n",
    "    filename = last_uploaded_file['name']\n",
    "    content = last_uploaded_file['content']\n",
    "    \n",
    "    # íŒŒì¼ ì €ì¥\n",
    "    save_dir = '/content/ComfyUI/input'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_uploaded = os.path.join(save_dir, filename)\n",
    "    \n",
    "    with open(file_uploaded, 'wb') as f:\n",
    "        f.write(content)\n",
    "        \n",
    "    print(f\"âœ… Image '{filename}' uploaded successfully to '{file_uploaded}'\")\n",
    "    \n",
    "    # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ í‘œì‹œ (ì˜µì…˜)\n",
    "    if ui.display_upload_check.value:\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
    "            img_data = io.BytesIO(content)\n",
    "            img = Image.open(img_data)\n",
    "            display(img)\n",
    "        else:\n",
    "            print(\"Cannot display this file type.\")\n",
    "    \n",
    "    # --- ìˆ˜ì •ëœ ë¶€ë¶„: ìœ„ì ¯ ê°’ì„ ìƒˆë¡œìš´ ë¹ˆ íŠœí”Œë¡œ ì„¤ì •í•˜ì—¬ ì´ˆê¸°í™” ---\n",
    "    ui.image_uploader.value = ()\n",
    "    \n",
    "    # ìœ„ì ¯ì˜ ë‚´ë¶€ ìƒíƒœë„ ì´ˆê¸°í™”í•˜ì—¬ ì¬ì—…ë¡œë“œê°€ ê°€ëŠ¥í•˜ê²Œ í•¨\n",
    "    ui.image_uploader._counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a23e23-d4c5-4776-a415-c82183bb3234",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ì…€ 4: ë¹„ë””ì˜¤ ìƒì„±\n",
    "import time\n",
    "import random\n",
    "\n",
    "if file_uploaded is None:\n",
    "    print(\"â€¼ï¸ ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì…€ 3ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- UI ê°’ ê°€ì ¸ì˜¤ê¸° ---\n",
    "    # ë¹„ë””ì˜¤ ì„¤ì •\n",
    "    positive_prompt_val = ui.positive_prompt.value\n",
    "    prompt_assist_val = ui.prompt_assist.value\n",
    "    prompt_assist_swap = swapT(prompt_assist_val, \"walking to camera\", \"walking to viewers\")\n",
    "    prompt_assist_swap = swapT(prompt_assist_swap, \"walking from camera\", \"walking from behind\")\n",
    "    prompt_assist_swap = swapT(prompt_assist_swap, \"swaying\", \"b3ll13-d8nc3r\")\n",
    "    final_positive_prompt = f\"{positive_prompt_val} {prompt_assist_swap}.\" if prompt_assist_swap != \"none\" else positive_prompt_val\n",
    "\n",
    "    # ì‹œë“œ ì„¤ì •\n",
    "    seed_val = ui.seed.value\n",
    "    if seed_val == 0:\n",
    "        seed_val = random.randint(0, 2**32 - 1)\n",
    "    print(f\"Using seed: {seed_val}\")\n",
    "    print(f\"prompt: {final_positive_prompt}\")\n",
    "    # ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ\n",
    "    generate_video(\n",
    "        image_path=file_uploaded,\n",
    "        LoRA_Strength=ui.lora_1_strength.value,\n",
    "        rel_l1_thresh=ui.rel_l1_thresh.value,\n",
    "        start_percent=ui.start_percent.value,\n",
    "        end_percent=ui.end_percent.value,\n",
    "        positive_prompt=final_positive_prompt,\n",
    "        prompt_assist=prompt_assist_swap,\n",
    "        negative_prompt=ui.negative_prompt.value,\n",
    "        width=ui.width.value,\n",
    "        height=ui.height.value,\n",
    "        seed=seed_val,\n",
    "        steps=ui.steps.value,\n",
    "        cfg_scale=ui.cfg_scale.value,\n",
    "        sampler_name=ui.sampler_name.value,\n",
    "        scheduler=ui.scheduler.value,\n",
    "        frames=ui.frames.value,\n",
    "        fps=16, # í•˜ë“œì½”ë”©ëœ ê°’\n",
    "        output_format=\"mp4\", # í•˜ë“œì½”ë”©ëœ ê°’\n",
    "        overwrite=ui.overwrite_previous_video.value,\n",
    "        use_lora=ui.use_lora_1.value,\n",
    "        use_lora2=ui.use_lora_2.value,\n",
    "        LoRA_Strength2=ui.lora_2_strength.value,\n",
    "        use_lora3=ui.use_lora_3.value,\n",
    "        LoRA_Strength3=ui.lora_3_strength.value,\n",
    "        use_lightx2v=ui.use_lightx2v.value,\n",
    "        lightx2v_Strength=ui.lightx2v_Strength.value,\n",
    "        lightx2v_steps=ui.steps.value, # steps ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "        use_pusa=ui.use_lightx2v2.value,\n",
    "        pusa_Strength=ui.lightx2v2_Strength.value,\n",
    "        pusa_steps=ui.steps.value, # steps ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "        use_sage_attention=ui.use_sage_attention.value,\n",
    "        enable_flow_shift=ui.use_flow_shift.value,\n",
    "        shift=ui.flow_shift.value,\n",
    "        enable_flow_shift2=ui.use_flow_shift.value,\n",
    "        shift2=ui.flow_shift2.value,\n",
    "        end_step1=ui.high_noise_steps.value\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    mins, secs = divmod(duration, 60)\n",
    "    print(f\"Seed: {seed_val}\")\n",
    "    print(f\"âœ… Generation completed in {int(mins)} min {secs:.2f} sec\")\n",
    "\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b5144-1d2c-46cc-bb22-ed92d87399c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ì…€ 5: í”„ë ˆì„ ë³´ê°„ (Frame Interpolation)\n",
    "import glob\n",
    "import time\n",
    "\n",
    "if not ui.interpolate_video.value:\n",
    "    print(\"â˜‘ï¸ Frame interpolation skipped as per UI setting.\")\n",
    "elif not output_path or not os.path.exists(output_path):\n",
    "    print(\"â€¼ï¸ No video file found from the previous step. Cannot interpolate.\")\n",
    "else:\n",
    "    print(\"âœ¨ Applying Frame Interpolation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # UIì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "    frame_multiplier_val = ui.frame_multiplier.value\n",
    "    interpolated_fps_val = ui.interpolated_fps.value\n",
    "    crf_val = ui.crf_value.value\n",
    "\n",
    "    print(f\"Original video path: {output_path}\")\n",
    "    print(f\"Converting video. Multiplier: {frame_multiplier_val}x, Target FPS: {interpolated_fps_val}, CRF: {crf_val}\")\n",
    "\n",
    "    # os.chdir ëŒ€ì‹  %cd ë§¤ì§ ì»¤ë§¨ë“œ ì‚¬ìš©\n",
    "    %cd /content/Practical-RIFE\n",
    "    \n",
    "    # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (os.system ëŒ€ì‹  ! ì‚¬ìš©)\n",
    "    inference_command = f\"python3 inference_video.py --multi={frame_multiplier_val} --fps={interpolated_fps_val} --video='{output_path}' --scale=1\"\n",
    "    !{inference_command}\n",
    "\n",
    "    # ê²°ê³¼ íŒŒì¼ ì°¾ê¸° ë° ë³€í™˜\n",
    "    video_folder = \"/content/ComfyUI/output/\"\n",
    "    # RIFEëŠ” ì›ë³¸ íŒŒì¼ëª…ì— _[multiplier]x.mp4 ë¥¼ ì¶”ê°€í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    base_name = os.path.splitext(os.path.basename(output_path))[0]\n",
    "    interpolated_video_path = os.path.join(video_folder, f\"{base_name}_{frame_multiplier_val}X_30fps.mp4\")\n",
    "\n",
    "    if os.path.exists(interpolated_video_path):\n",
    "        final_output_path = \"/content/Practical-RIFE/output_converted.mp4\"\n",
    "        # ffmpeg ëª…ë ¹ì–´ ì‹¤í–‰ (os.system ëŒ€ì‹  ! ì‚¬ìš©)\n",
    "        ffmpeg_command = f\"ffmpeg -i '{interpolated_video_path}' -vcodec libx264 -crf {crf_val} -preset fast '{final_output_path}' -loglevel error -y\"\n",
    "        !{ffmpeg_command}\n",
    "        \n",
    "        print(f\"Displaying final video: {final_output_path}\")\n",
    "        display_video(final_output_path)\n",
    "    else:\n",
    "        print(f\"âŒ Interpolated video not found at expected path: {interpolated_video_path}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    mins, secs = divmod(duration, 60)\n",
    "    print(f\"âœ… Frame Interpolation completed in {int(mins)} min {secs:.2f} sec\")\n",
    "\n",
    "    clear_memory()\n",
    "    # ì‘ì—… í›„ ì›ë˜ ë””ë ‰í† ë¦¬ë¡œ ëŒì•„ê°\n",
    "    %cd /content/ComfyUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c45b3-5894-4be2-b2ec-3304981b727b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ì…€ 6: íŒŒì¼ ì‚­ì œ\n",
    "!rm -R /content/ComfyUI/input/*\n",
    "!rm -R /content/ComfyUI/output/*\n",
    "print(\"ì‚­ì œì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051c1ca-08b8-4995-b0a6-ca10aa082599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
