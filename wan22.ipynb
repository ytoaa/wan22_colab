{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3eb4d-d9a1-47c5-b881-c380c2e7d3cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 셀 1: 사용자 인터페이스 (UI) 설정\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Google Colab의 display와 ipywidgets의 display가 충돌할 수 있으므로, 명시적으로 스타일을 지정합니다.\n",
    "display(HTML(\"<style>.widget-label { min-width: 25ex !important; }</style>\"))\n",
    "\n",
    "class UI:\n",
    "    def __init__(self):\n",
    "        # --- 모델 설정 ---\n",
    "        self.model_quant = widgets.Dropdown(options=[\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\"], value=\"Q4_K_M\", description=\"Model Quantization:\")\n",
    "        self.lightx2v_rank = widgets.Dropdown(options=[\"32\", \"64\", \"128\"], value=\"32\", description=\"Lightx2v Rank:\")\n",
    "\n",
    "        # --- LoRA 다운로드 설정 ---\n",
    "        self.download_lora_1 = widgets.Checkbox(value=False, description=\"Download LoRA 1\")\n",
    "        self.lora_1_url = widgets.Text(value=\"Put your LoRA URL here\", description=\"LoRA 1 URL:\")\n",
    "        self.download_lora_2 = widgets.Checkbox(value=False, description=\"Download LoRA 2\")\n",
    "        self.lora_2_url = widgets.Text(value=\"Put your LoRA URL here\", description=\"LoRA 2 URL:\")\n",
    "        self.download_lora_3 = widgets.Checkbox(value=False, description=\"Download LoRA 3\")\n",
    "        self.lora_3_url = widgets.Text(value=\"https://huggingface.co/Remade-AI/Rotate/resolve/main/rotate_20_epochs.safetensors\", description=\"LoRA 3 URL:\")\n",
    "        self.civitai_token = widgets.Password(description=\"Civitai Token:\", value=\"\")\n",
    "\n",
    "        # --- 비디오 생성 설정 ---\n",
    "        self.positive_prompt = widgets.Textarea(value=\"slow jumping and fast dancing\", description=\"Positive Prompt:\", layout={'width': '95%', 'height': '80px'})\n",
    "        self.prompt_assist = widgets.Dropdown(options=[\"none\", \"walking to camera\", \"walking from camera\", \"swaying\"], value=\"none\", description=\"Prompt Assist:\")\n",
    "        self.negative_prompt = widgets.Textarea(value=\"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走\", description=\"Negative Prompt:\", layout={'width': '95%', 'height': '80px'})\n",
    "        self.width = widgets.IntText(value=720, description=\"Width:\")\n",
    "        self.height = widgets.IntText(value=1280, description=\"Height:\")\n",
    "        self.seed = widgets.IntText(value=0, description=\"Seed (0=random):\")\n",
    "        self.high_noise_steps = widgets.IntSlider(value=3, min=1, max=25, step=1, description=\"High Noise Steps:\")\n",
    "        self.steps = widgets.IntSlider(value=6, min=1, max=50, step=1, description=\"Total Steps:\")\n",
    "        self.cfg_scale = widgets.FloatSlider(value=1.0, min=1.0, max=20.0, step=0.1, description=\"CFG Scale:\")\n",
    "        self.sampler_name = widgets.Dropdown(options=[\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"], value=\"euler\", description=\"Sampler:\")\n",
    "        self.scheduler = widgets.Dropdown(options=[\"simple\",\"normal\",\"karras\",\"exponential\",\"sgm_uniform\",\"ddim_uniform\",\"beta\",\"linear_quadratic\",\"kl_optimal\"], value=\"simple\", description=\"Scheduler:\")\n",
    "        self.frames = widgets.IntSlider(value=81, min=1, max=120, step=1, description=\"Frames:\")\n",
    "        self.overwrite_previous_video = widgets.Checkbox(value=True, description=\"Overwrite Previous Video\")\n",
    "\n",
    "        # --- 모델 고급 설정 ---\n",
    "        self.use_sage_attention = widgets.Checkbox(value=True, description=\"Use Sage Attention:\")\n",
    "        self.use_flow_shift = widgets.Checkbox(value=True, description=\"Use Flow Shift:\")\n",
    "        self.flow_shift = widgets.FloatSlider(value=8.0, min=0.0, max=100.0, step=0.01, description=\"Flow Shift 1:\")\n",
    "        self.flow_shift2 = widgets.FloatSlider(value=8.0, min=0.0, max=100.0, step=0.01, description=\"Flow Shift 2:\")\n",
    "\n",
    "        # --- Wan 2.1 LoRA 설정 ---\n",
    "        self.use_lightx2v = widgets.Checkbox(value=True, description=\"Use Lightx2v:\")\n",
    "        self.lightx2v_Strength = widgets.FloatSlider(value=3.0, min=-10.0, max=10.0, step=0.01, description=\"Lightx2v Strength:\")\n",
    "        self.use_lightx2v2 = widgets.Checkbox(value=True, description=\"Use Lightx2v2 (Pusa):\")\n",
    "        self.lightx2v2_Strength = widgets.FloatSlider(value=1.5, min=-10.0, max=10.0, step=0.01, description=\"Lightx2v2 Strength:\")\n",
    "\n",
    "        # --- 커스텀 LoRA 설정 ---\n",
    "        self.use_lora_1 = widgets.Checkbox(value=False, description=\"Use Custom LoRA 1:\")\n",
    "        self.lora_1_strength = widgets.FloatSlider(value=1.0, min=-10.0, max=10.0, step=0.01, description=\"LoRA 1 Strength:\")\n",
    "        self.use_lora_2 = widgets.Checkbox(value=False, description=\"Use Custom LoRA 2:\")\n",
    "        self.lora_2_strength = widgets.FloatSlider(value=1.0, min=-10.0, max=10.0, step=0.01, description=\"LoRA 2 Strength:\")\n",
    "        self.use_lora_3 = widgets.Checkbox(value=False, description=\"Use Custom LoRA 3:\")\n",
    "        self.lora_3_strength = widgets.FloatSlider(value=1.0, min=-10.0, max=10.0, step=0.01, description=\"LoRA 3 Strength:\")\n",
    "\n",
    "        # --- Teacache 설정 ---\n",
    "        self.rel_l1_thresh = widgets.FloatSlider(value=0, min=0.0, max=10.0, step=0.001, description=\"Rel L1 Thresh:\")\n",
    "        self.start_percent = widgets.FloatSlider(value=0.2, min=0.0, max=1.0, step=0.01, description=\"Start Percent:\")\n",
    "        self.end_percent = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.01, description=\"End Percent:\")\n",
    "        \n",
    "        # --- 프레임 보간 설정 ---\n",
    "        self.interpolate_video = widgets.Checkbox(value=True, description=\"Apply Frame Interpolation\")\n",
    "        self.frame_multiplier = widgets.IntText(value=2, description=\"Frame Multiplier:\")\n",
    "        self.interpolated_fps = widgets.IntText(value=30, description=\"Interpolated FPS:\")\n",
    "        self.crf_value = widgets.IntSlider(value=17, min=0, max=51, step=1, description=\"CRF (Quality):\")\n",
    "        \n",
    "        # --- 파일 업로드 ---\n",
    "        self.image_uploader = widgets.FileUpload(accept='image/*', description='Upload Image')\n",
    "        self.display_upload_check = widgets.Checkbox(value=False, description=\"Display uploaded image\")\n",
    "\n",
    "\n",
    "    def display_ui(self):\n",
    "        lora_downloads = widgets.VBox([\n",
    "            self.download_lora_1, self.lora_1_url,\n",
    "            self.download_lora_2, self.lora_2_url,\n",
    "            self.download_lora_3, self.lora_3_url,\n",
    "            self.civitai_token\n",
    "        ])\n",
    "        \n",
    "        generation_settings = widgets.VBox([\n",
    "            self.positive_prompt, self.prompt_assist, self.negative_prompt,\n",
    "            widgets.HBox([self.width, self.height]),\n",
    "            self.seed, self.high_noise_steps, self.steps, self.cfg_scale,\n",
    "            self.sampler_name, self.scheduler, self.frames, self.overwrite_previous_video\n",
    "        ])\n",
    "        \n",
    "        model_config = widgets.VBox([\n",
    "            self.use_sage_attention,\n",
    "            widgets.HBox([self.use_flow_shift, self.flow_shift, self.flow_shift2])\n",
    "        ])\n",
    "\n",
    "        wan_lora_config = widgets.VBox([\n",
    "            widgets.HBox([self.use_lightx2v, self.lightx2v_Strength]),\n",
    "            widgets.HBox([self.use_lightx2v2, self.lightx2v2_Strength])\n",
    "        ])\n",
    "        \n",
    "        custom_lora_config = widgets.VBox([\n",
    "            widgets.HBox([self.use_lora_1, self.lora_1_strength]),\n",
    "            widgets.HBox([self.use_lora_2, self.lora_2_strength]),\n",
    "            widgets.HBox([self.use_lora_3, self.lora_3_strength])\n",
    "        ])\n",
    "\n",
    "        teacache_settings = widgets.VBox([\n",
    "            self.rel_l1_thresh, self.start_percent, self.end_percent\n",
    "        ])\n",
    "        \n",
    "        interpolation_settings = widgets.VBox([\n",
    "            self.interpolate_video, self.frame_multiplier, self.interpolated_fps, self.crf_value\n",
    "        ])\n",
    "\n",
    "        accordion = widgets.Accordion(children=[\n",
    "            generation_settings,\n",
    "            widgets.VBox([self.model_quant, self.lightx2v_rank]),\n",
    "            lora_downloads,\n",
    "            model_config,\n",
    "            wan_lora_config,\n",
    "            custom_lora_config,\n",
    "            teacache_settings,\n",
    "            interpolation_settings\n",
    "        ])\n",
    "        \n",
    "        accordion.set_title(0, '📝 Video Settings')\n",
    "        accordion.set_title(1, '🤖 Base Model Settings')\n",
    "        accordion.set_title(2, '📥 LoRA Downloads')\n",
    "        accordion.set_title(3, '⚙️ Model Advanced Config')\n",
    "        accordion.set_title(4, '🚀 Wan 2.1 LoRA Config')\n",
    "        accordion.set_title(5, '🎨 Custom LoRA Config')\n",
    "        accordion.set_title(6, '🧠 Teacache Settings')\n",
    "        accordion.set_title(7, '✨ Frame Interpolation')\n",
    "\n",
    "        upload_box = widgets.VBox([self.image_uploader, self.display_upload_check])\n",
    "        \n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h1>💥 ComfyUI Video Generation Settings</h1>\"),\n",
    "            widgets.HTML(\"<h3>1. 파일 업로드</h3>\"),\n",
    "            upload_box,\n",
    "            widgets.HTML(\"<hr><h3>2. 생성 옵션</h3>\"),\n",
    "            accordion\n",
    "        ]))\n",
    "\n",
    "# UI 인스턴스 생성 및 표시\n",
    "ui = UI()\n",
    "ui.display_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d3c7d-bacf-492c-b95a-7fc6465aeaf2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 셀 2: 사전설정\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output, display, HTML, Image as IPImage\n",
    "import random\n",
    "\n",
    "# 1. 라이브러리 설치\n",
    "print(\"Installing required libraries...\")\n",
    "!pip install torch==2.6.0 torchvision==0.21.0\n",
    "!pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2 triton==3.2.0 sageattention\n",
    "!pip install -q av spandrel albumentations insightface onnx opencv-python segment_anything ultralytics onnxruntime onnxruntime-gpu\n",
    "!apt -y install -qq aria2 ffmpeg\n",
    "clear_output(wait=True)\n",
    "print(\"Library installation complete.\")\n",
    "\n",
    "# 2. ComfyUI 및 커스텀 노드 클론 및 설정\n",
    "print(\"Setting up ComfyUI and custom nodes...\")\n",
    "if not os.path.exists('/content/ComfyUI'):\n",
    "    !git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\n",
    "    %cd /content/ComfyUI/custom_nodes\n",
    "    !git clone https://github.com/city96/ComfyUI-GGUF ComfyUI_GGUF\n",
    "    !git clone https://github.com/kijai/ComfyUI-KJNodes ComfyUI_KJNodes\n",
    "    %cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
    "    !pip install -r requirements.txt\n",
    "    %cd /content/ComfyUI/custom_nodes/ComfyUI_KJNodes\n",
    "    !sed -i 's/^/#/' /content/ComfyUI/custom_nodes/ComfyUI_KJNodes/__init__.py\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "# 3. RIFE 모델 설치\n",
    "if not os.path.exists('/content/Practical-RIFE'):\n",
    "    %cd /content\n",
    "    !git clone https://github.com/Isi-dev/Practical-RIFE\n",
    "    %cd /content/Practical-RIFE\n",
    "    !pip install git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation\n",
    "    os.makedirs('/content/Practical-RIFE/train_log', exist_ok=True)\n",
    "    !wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/IFNet_HDv3.py -O /content/Practical-RIFE/train_log/IFNet_HDv3.py\n",
    "    !wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/RIFE_HDv3.py -O /content/Practical-RIFE/train_log/RIFE_HDv3.py\n",
    "    !wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/refine.py -O /content/Practical-RIFE/train_log/refine.py\n",
    "    !wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/flownet.pkl -O /content/Practical-RIFE/train_log/flownet.pkl\n",
    "\n",
    "%cd /content/ComfyUI\n",
    "!sed -i -e 's/^from server import PromptServer/# from server import PromptServer/' -e '/^\\\\s*if unique_id:/s/^/    # /' -e '/PromptServer\\\\.instance\\\\.send_progress_text/s/^/            # /' /content/ComfyUI/comfy_extras/nodes_images.py\n",
    "clear_output(wait=True)\n",
    "print(\"ComfyUI setup complete.\")\n",
    "\n",
    "# 4. 파이썬 환경 설정 및 모듈 임포트\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "sys.path.insert(0, '/content/ComfyUI')\n",
    "\n",
    "from comfy import model_management\n",
    "from nodes import CheckpointLoaderSimple, CLIPLoader, CLIPTextEncode, VAEDecode, VAELoader, KSampler, KSamplerAdvanced, UNETLoader, LoadImage, SaveImage, CLIPVisionLoader, CLIPVisionEncode, LoraLoaderModelOnly, ImageScale\n",
    "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
    "from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import WanVideoTeaCacheKJ, PathchSageAttentionKJ, WanVideoNAG, SkipLayerGuidanceWanVideo\n",
    "from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
    "from comfy_extras.nodes_images import SaveAnimatedWEBP\n",
    "from comfy_extras.nodes_video import SaveWEBM\n",
    "from comfy_extras.nodes_wan import WanImageToVideo\n",
    "from comfy_extras.nodes_upscale_model import UpscaleModelLoader\n",
    "\n",
    "# --- START OF ALL HELPER FUNCTIONS ---\n",
    "# 5. 유틸리티 함수 정의 (수정됨)\n",
    "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
    "    import os\n",
    "    filename = link.split(\"/\")[-1]\n",
    "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
    "    print(\"Executing download command:\")\n",
    "    print(command)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    get_ipython().system(command) # os.system 대신 사용\n",
    "    return filename\n",
    "\n",
    "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
    "    import os\n",
    "    import time\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    try:\n",
    "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
    "    except IndexError:\n",
    "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
    "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
    "    if civitai_token:\n",
    "        civitai_url += f\"&token={civitai_token}\"\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"model_{timestamp}.safetensors\"\n",
    "    full_path = os.path.join(folder, filename)\n",
    "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
    "    print(\"Downloading from Civitai...\")\n",
    "    get_ipython().system(download_command) # os.system 대신 사용\n",
    "    local_path = os.path.join(folder, filename)\n",
    "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
    "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
    "    else:\n",
    "        print(f\"❌ LoRA download failed or file is empty: {local_path}\")\n",
    "    return filename\n",
    "\n",
    "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
    "    if \"civitai.com\" in link.lower():\n",
    "        if not civitai_token:\n",
    "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
    "        return download_civitai_model(link, civitai_token, folder)\n",
    "    else:\n",
    "        return download_with_aria2c(link, folder)\n",
    "\n",
    "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
    "    try:\n",
    "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
    "        if filename is None:\n",
    "            filename = url.split('/')[-1].split('?')[0]\n",
    "        cmd = ['aria2c', '--console-log-level=error', '-c', '-x', '16', '-s', '16', '-k', '1M', '-d', dest_dir, '-o', filename, url]\n",
    "        if silent:\n",
    "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
    "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
    "        subprocess.run(cmd, check=True, capture_output=silent, text=True) # subprocess는 그대로 사용해도 좋습니다.\n",
    "        if silent:\n",
    "            print(\"Done!\")\n",
    "        else:\n",
    "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
    "        return filename\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\nError downloading {filename}: {e.stderr.strip() if e.stderr else 'Unknown error'}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def image_width_height(image):\n",
    "    if image.ndim == 4: _, height, width, _ = image.shape\n",
    "    elif image.ndim == 3: height, width, _ = image.shape\n",
    "    else: raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
    "    return width, height\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
    "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
    "    with imageio.get_writer(output_path, fps=fps) as writer:\n",
    "        for frame in frames: writer.append_data(frame)\n",
    "    return output_path\n",
    "\n",
    "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
    "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
    "    Image.fromarray(frame).save(output_path)\n",
    "    return output_path\n",
    "\n",
    "def save_as_webm(images, filename_prefix, fps, codec=\"vp9\", quality=32, output_dir=\"/content/ComfyUI/output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True); output_path = f\"{output_dir}/{filename_prefix}.webm\"\n",
    "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
    "    kwargs = {'fps': int(fps), 'quality': int(quality), 'codec': str(codec), 'output_params': ['-crf', str(int(quality))]}\n",
    "    with imageio.get_writer(output_path, format='FFMPEG', mode='I', **kwargs) as writer:\n",
    "        for frame in frames: writer.append_data(frame)\n",
    "    return output_path\n",
    "\n",
    "def display_video(video_path):\n",
    "    from base64 import b64encode\n",
    "    video_data = open(video_path,'rb').read()\n",
    "    mime_type = f\"video/{video_path.split('.')[-1]}\"\n",
    "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
    "    display(HTML(f'<video width=512 controls autoplay loop><source src=\"{data_url}\" type=\"{mime_type}\"></video>'))\n",
    "\n",
    "# --- 누락되었던 swapT 함수 추가 ---\n",
    "def swapT(pa, f, s):\n",
    "    return s if pa == f else pa\n",
    "\n",
    "output_path = \"\"\n",
    "output_pathU = \"\"\n",
    "\n",
    "# ======================================================================\n",
    "# ========= THE MAIN GENERATE_VIDEO FUNCTION STARTS HERE ===============\n",
    "# ======================================================================\n",
    "def generate_video(\n",
    "    image_path: str = None,\n",
    "    LoRA_Strength: float = 1.00,\n",
    "    rel_l1_thresh: float = 0.275,\n",
    "    start_percent: float = 0.1,\n",
    "    end_percent: float = 1.0,\n",
    "    positive_prompt: str = \"a cute anime girl with massive fennec ears and a big fluffy tail wearing a maid outfit turning around\",\n",
    "    prompt_assist: str = \"walking to viewers\",\n",
    "    negative_prompt: str = \"...\",\n",
    "    width: int = 832,\n",
    "    height: int = 480,\n",
    "    seed: int = 82628696717253,\n",
    "    steps: int = 20,\n",
    "    cfg_scale: float = 1.0,\n",
    "    sampler_name: str = \"uni_pc\",\n",
    "    scheduler: str = \"simple\",\n",
    "    frames: int = 33,\n",
    "    fps: int = 16,\n",
    "    output_format: str = \"mp4\",\n",
    "    overwrite: bool = False,\n",
    "    use_lora: bool = True,\n",
    "    use_lora2: bool = True,\n",
    "    LoRA_Strength2: float = 1.00,\n",
    "    use_lora3: bool = True,\n",
    "    LoRA_Strength3: float = 1.00,\n",
    "    use_lightx2v: bool = False,\n",
    "    lightx2v_Strength: float = 0.80,\n",
    "    lightx2v_steps: int = 4,\n",
    "    use_pusa: bool = False,\n",
    "    pusa_Strength: float = 1.2,\n",
    "    pusa_steps: int = 6,\n",
    "    use_sage_attention: bool = True,\n",
    "    enable_flow_shift: bool = True,\n",
    "    shift: float = 8.0,\n",
    "    enable_flow_shift2: bool = True,\n",
    "    shift2: float = 8.0,\n",
    "    end_step1: int = 10,\n",
    "):\n",
    "    with torch.inference_mode():\n",
    "        unet_loader = UnetLoaderGGUF(); pathch_sage_attention = PathchSageAttentionKJ(); wan_video_nag = WanVideoNAG(); teacache = WanVideoTeaCacheKJ(); model_sampling = ModelSamplingSD3(); clip_loader = CLIPLoader(); clip_encode_positive = CLIPTextEncode(); clip_encode_negative = CLIPTextEncode(); vae_loader = VAELoader(); clip_vision_loader = CLIPVisionLoader(); clip_vision_encode = CLIPVisionEncode(); load_image = LoadImage(); wan_image_to_video = WanImageToVideo(); ksampler = KSamplerAdvanced(); vae_decode = VAEDecode(); save_webp = SaveAnimatedWEBP(); save_webm = SaveWEBM(); pAssLora = LoraLoaderModelOnly(); load_lora = LoraLoaderModelOnly(); load_lora2 = LoraLoaderModelOnly(); load_lora3 = LoraLoaderModelOnly(); load_lightx2v_lora = LoraLoaderModelOnly(); load_pusa_lora = LoraLoaderModelOnly(); image_scaler = ImageScale()\n",
    "        print(\"Loading Text_Encoder...\"); clip = clip_loader.load_clip(\"umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"wan\", \"default\")[0]; positive = clip_encode_positive.encode(clip, positive_prompt)[0]; negative = clip_encode_negative.encode(clip, negative_prompt)[0]; del clip; torch.cuda.empty_cache(); gc.collect()\n",
    "        if image_path is None: print(\"No image uploaded!\"); return\n",
    "        loaded_image = load_image.load_image(image_path)[0]; width_int, height_int = image_width_height(loaded_image)\n",
    "        if height == 0: height = int(width * height_int / width_int)\n",
    "        print(f\"Image resolution is {width_int}x{height_int}\"); print(f\"Scaling image to {width}x{height}...\")\n",
    "        loaded_image = image_scaler.upscale(loaded_image, \"lanczos\", width, height, \"disabled\")[0]\n",
    "        clip_vision_output = None\n",
    "        print(\"Loading VAE...\"); vae = vae_loader.load_vae(\"wan_2.1_vae.safetensors\")[0]\n",
    "        positive_out, negative_out, latent = wan_image_to_video.encode(positive, negative, vae, width, height, frames, 1, loaded_image, clip_vision_output)\n",
    "        usedSteps = steps\n",
    "        print(\"Loading high noise Model...\"); model = unet_loader.load_unet(dit_model)[0]\n",
    "        if enable_flow_shift: model = model_sampling.patch(model, shift)[0]\n",
    "        if prompt_assist != \"none\":\n",
    "            if prompt_assist == \"walking to viewers\": print(\"Loading walking to camera LoRA...\"); model = pAssLora.load_lora_model_only(model, walkingToViewersL, 1)[0]\n",
    "            if prompt_assist == \"walking from behind\": print(\"Loading walking from camera LoRA...\"); model = pAssLora.load_lora_model_only(model, walkingFromBehindL, 1)[0]\n",
    "            if prompt_assist == \"b3ll13-d8nc3r\": print(\"Loading dancing LoRA...\"); model = pAssLora.load_lora_model_only(model, dancingL, 1)[0]\n",
    "        if use_lora and lora_1 is not None: print(\"Loading LoRA...\"); model = load_lora.load_lora_model_only(model, lora_1, LoRA_Strength)[0]\n",
    "        if use_lora2 and lora_2 is not None: print(\"Loading LoRA 2...\"); model = load_lora2.load_lora_model_only(model, lora_2, LoRA_Strength2)[0]\n",
    "        if use_lora3 and lora_3 is not None: print(\"Loading LoRA 3...\"); model = load_lora3.load_lora_model_only(model, lora_3, LoRA_Strength3)[0]\n",
    "        if use_lightx2v: print(\"Loading lightx2v LoRA...\"); model = load_lightx2v_lora.load_lora_model_only(model, lightx2v_lora, lightx2v_Strength)[0]; usedSteps=lightx2v_steps\n",
    "        if use_sage_attention: model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
    "        if rel_l1_thresh > 0: print(\"Setting Teacache...\"); model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"14B\")[0]\n",
    "        clear_output()\n",
    "        print(\"Generating video with high noise model...\"); sampled = ksampler.sample(model=model, add_noise=\"enable\", noise_seed=seed, steps=usedSteps, cfg=cfg_scale, sampler_name=sampler_name, scheduler=scheduler, positive=positive_out, negative=negative_out, latent_image=latent, start_at_step=0, end_at_step=end_step1, return_with_leftover_noise=\"enable\")[0]\n",
    "        del model; torch.cuda.empty_cache(); gc.collect()\n",
    "        print(\"Loading low noise Model...\"); model = unet_loader.load_unet(dit_model2)[0]\n",
    "        if enable_flow_shift2: model = model_sampling.patch(model, shift2)[0]\n",
    "        if prompt_assist != \"none\":\n",
    "            if prompt_assist == \"walking to viewers\": print(\"Loading walking to camera LoRA...\"); model = pAssLora.load_lora_model_only(model, walkingToViewersL, 1)[0]\n",
    "            if prompt_assist == \"walking from behind\": print(\"Loading walking from camera LoRA...\"); model = pAssLora.load_lora_model_only(model, walkingFromBehindL, 1)[0]\n",
    "            if prompt_assist == \"b3ll13-d8nc3r\": print(\"Loading dancing LoRA...\"); model = pAssLora.load_lora_model_only(model, dancingL, 1)[0]\n",
    "        if use_lora and lora_1 is not None: print(\"Loading LoRA...\"); model = load_lora.load_lora_model_only(model, lora_1, LoRA_Strength)[0]\n",
    "        if use_lora2 and lora_2 is not None: print(\"Loading LoRA 2...\"); model = load_lora2.load_lora_model_only(model, lora_2, LoRA_Strength2)[0]\n",
    "        if use_lora3 and lora_3 is not None: print(\"Loading LoRA 3...\"); model = load_lora3.load_lora_model_only(model, lora_3, LoRA_Strength3)[0]\n",
    "        if use_pusa: print(\"Loading pusav1 LoRA...\"); model = load_pusa_lora.load_lora_model_only(model, lightx2v_lora, pusa_Strength)[0]; usedSteps=pusa_steps\n",
    "        if use_sage_attention: model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
    "        if rel_l1_thresh > 0: print(\"Setting Teacache...\"); model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"14B\")[0]\n",
    "        clear_output()\n",
    "        print(\"Generating video with low noise model...\"); sampled = ksampler.sample(model=model, add_noise=\"disable\", noise_seed=seed, steps=usedSteps, cfg=cfg_scale, sampler_name=sampler_name, scheduler=scheduler, positive=positive_out, negative=negative_out, latent_image=sampled, start_at_step=end_step1, end_at_step=10000, return_with_leftover_noise=\"disable\")[0]\n",
    "        del model; torch.cuda.empty_cache(); gc.collect()\n",
    "        try:\n",
    "            print(\"Decoding latents...\"); decoded = vae_decode.decode(vae, sampled)[0]; del vae; torch.cuda.empty_cache(); gc.collect()\n",
    "            global output_path; import datetime; base_name = \"ComfyUI\"\n",
    "            if not overwrite: base_name += f\"_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            if frames == 1: print(\"Single frame, saving as PNG...\"); output_path = save_as_image(decoded[0], \"ComfyUI\"); display(IPImage(filename=output_path))\n",
    "            else:\n",
    "                if output_format.lower() == \"webm\": print(\"Saving as WEBM...\"); output_path = save_as_webm(decoded, base_name, fps=fps, codec=\"vp9\", quality=10)\n",
    "                elif output_format.lower() == \"mp4\": print(\"Saving as MP4...\"); output_path = save_as_mp4(decoded, base_name, fps)\n",
    "                else: raise ValueError(f\"Unsupported format: {output_format}\")\n",
    "                display_video(output_path)\n",
    "        except Exception as e: print(f\"Error during decoding/saving: {str(e)}\"); raise\n",
    "        finally: clear_memory()\n",
    "\n",
    "# --- END OF ALL HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "# 6. UI 설정에 따라 모델 다운로드 (이 부분은 UI 객체 'ui'가 정의되어 있어야 합니다)\n",
    "# 아래 코드는 'ui'라는 객체가 이전에 정의되었다고 가정합니다.\n",
    "# 만약 'ui' 객체가 없다면, 이 부분은 실행되지 않거나 오류를 발생시킵니다.\n",
    "\n",
    "try:\n",
    "    print(\"Downloading models based on UI settings...\")\n",
    "\n",
    "    # LoRA 다운로드\n",
    "    lora_1, lora_2, lora_3 = None, None, None\n",
    "    if ui.download_lora_1.value:\n",
    "        lora_1 = download_lora(ui.lora_1_url.value, civitai_token=ui.civitai_token.value)\n",
    "    if ui.download_lora_2.value:\n",
    "        lora_2 = download_lora(ui.lora_2_url.value, civitai_token=ui.civitai_token.value)\n",
    "    if ui.download_lora_3.value:\n",
    "        lora_3 = download_lora(ui.lora_3_url.value, civitai_token=ui.civitai_token.value)\n",
    "    clear_output(wait=True)\n",
    "    print(\"LoRA download process finished.\")\n",
    "\n",
    "    # 메인 모델 (Dit) 다운로드\n",
    "    model_quant_val = ui.model_quant.value\n",
    "    if model_quant_val == \"Q4_K_M\":\n",
    "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "        dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "    elif model_quant_val == \"Q5_K_M\":\n",
    "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "        dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "    elif model_quant_val == \"Q6_K\":\n",
    "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "        dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "    else: # Q8_0\n",
    "        dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "        dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
    "\n",
    "    # 필수 모델 다운로드\n",
    "    model_download(\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"/content/ComfyUI/models/text_encoders\")\n",
    "    model_download(\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors\", \"/content/ComfyUI/models/vae\")\n",
    "    model_download(\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors\", \"/content/ComfyUI/models/clip_vision\")\n",
    "\n",
    "    # Lightx2v LoRA 다운로드\n",
    "    lightx2v_rank_val = ui.lightx2v_rank.value\n",
    "    if lightx2v_rank_val == \"32\":\n",
    "        lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_I2V_14B_480p_cfg_step_distill_rank32_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "    elif lightx2v_rank_val == \"64\":\n",
    "        lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank64_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "    else: # 128\n",
    "        lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank128_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "\n",
    "    # 보조 LoRA 다운로드\n",
    "    walkingToViewersL = model_download(\"https://huggingface.co/Isi99999/Wan2.1_14B-480p_I2V_LoRAs/resolve/main/walking%20to%20viewers_Wan.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "    walkingFromBehindL = model_download(\"https://huggingface.co/Isi99999/Wan2.1_14B-480p_I2V_LoRAs/resolve/main/walking_from_behind.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "    dancingL = model_download(\"https://huggingface.co/Isi99999/Wan2.1_14B-480p_I2V_LoRAs/resolve/main/b3ll13-d8nc3r.safetensors\", \"/content/ComfyUI/models/loras\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"✅ Environment Setup and Model Download Complete!\")\n",
    "    file_uploaded = None # 업로드된 파일 경로를 저장할 변수\n",
    "\n",
    "except NameError:\n",
    "    print(\"UI object 'ui' is not defined. Skipping UI-based model downloads.\")\n",
    "    print(\"Please define the 'ui' object or download models manually if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3559ac9-a648-49da-8248-e507faa79f65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 셀 3: 이미지 업로드 (두 번째 오류 수정 버전)\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# UI 위젯에서 업로드된 파일 정보 가져오기\n",
    "uploaded_file_info = ui.image_uploader.value\n",
    "\n",
    "if not uploaded_file_info:\n",
    "    print(\"‼️ 이미지를 먼저 업로드해주세요 (셀 1의 'Upload Image' 버튼 사용).\")\n",
    "else:\n",
    "    # 마지막(첫 번째)으로 업로드된 파일을 사용합니다.\n",
    "    last_uploaded_file = uploaded_file_info[0]\n",
    "    \n",
    "    filename = last_uploaded_file['name']\n",
    "    content = last_uploaded_file['content']\n",
    "    \n",
    "    # 파일 저장\n",
    "    save_dir = '/content/ComfyUI/input'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_uploaded = os.path.join(save_dir, filename)\n",
    "    \n",
    "    with open(file_uploaded, 'wb') as f:\n",
    "        f.write(content)\n",
    "        \n",
    "    print(f\"✅ Image '{filename}' uploaded successfully to '{file_uploaded}'\")\n",
    "    \n",
    "    # 업로드된 이미지 표시 (옵션)\n",
    "    if ui.display_upload_check.value:\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
    "            img_data = io.BytesIO(content)\n",
    "            img = Image.open(img_data)\n",
    "            display(img)\n",
    "        else:\n",
    "            print(\"Cannot display this file type.\")\n",
    "    \n",
    "    # --- 수정된 부분: 위젯 값을 새로운 빈 튜플로 설정하여 초기화 ---\n",
    "    ui.image_uploader.value = ()\n",
    "    \n",
    "    # 위젯의 내부 상태도 초기화하여 재업로드가 가능하게 함\n",
    "    ui.image_uploader._counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a23e23-d4c5-4776-a415-c82183bb3234",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 셀 4: 비디오 생성\n",
    "import time\n",
    "import random\n",
    "\n",
    "if file_uploaded is None:\n",
    "    print(\"‼️ 이미지가 업로드되지 않았습니다. 셀 3을 먼저 실행해주세요.\")\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- UI 값 가져오기 ---\n",
    "    # 비디오 설정\n",
    "    positive_prompt_val = ui.positive_prompt.value\n",
    "    prompt_assist_val = ui.prompt_assist.value\n",
    "    prompt_assist_swap = swapT(prompt_assist_val, \"walking to camera\", \"walking to viewers\")\n",
    "    prompt_assist_swap = swapT(prompt_assist_swap, \"walking from camera\", \"walking from behind\")\n",
    "    prompt_assist_swap = swapT(prompt_assist_swap, \"swaying\", \"b3ll13-d8nc3r\")\n",
    "    final_positive_prompt = f\"{positive_prompt_val} {prompt_assist_swap}.\" if prompt_assist_swap != \"none\" else positive_prompt_val\n",
    "\n",
    "    # 시드 설정\n",
    "    seed_val = ui.seed.value\n",
    "    if seed_val == 0:\n",
    "        seed_val = random.randint(0, 2**32 - 1)\n",
    "    print(f\"Using seed: {seed_val}\")\n",
    "    print(f\"prompt: {final_positive_prompt}\")\n",
    "    # 생성 함수 호출\n",
    "    generate_video(\n",
    "        image_path=file_uploaded,\n",
    "        LoRA_Strength=ui.lora_1_strength.value,\n",
    "        rel_l1_thresh=ui.rel_l1_thresh.value,\n",
    "        start_percent=ui.start_percent.value,\n",
    "        end_percent=ui.end_percent.value,\n",
    "        positive_prompt=final_positive_prompt,\n",
    "        prompt_assist=prompt_assist_swap,\n",
    "        negative_prompt=ui.negative_prompt.value,\n",
    "        width=ui.width.value,\n",
    "        height=ui.height.value,\n",
    "        seed=seed_val,\n",
    "        steps=ui.steps.value,\n",
    "        cfg_scale=ui.cfg_scale.value,\n",
    "        sampler_name=ui.sampler_name.value,\n",
    "        scheduler=ui.scheduler.value,\n",
    "        frames=ui.frames.value,\n",
    "        fps=16, # 하드코딩된 값\n",
    "        output_format=\"mp4\", # 하드코딩된 값\n",
    "        overwrite=ui.overwrite_previous_video.value,\n",
    "        use_lora=ui.use_lora_1.value,\n",
    "        use_lora2=ui.use_lora_2.value,\n",
    "        LoRA_Strength2=ui.lora_2_strength.value,\n",
    "        use_lora3=ui.use_lora_3.value,\n",
    "        LoRA_Strength3=ui.lora_3_strength.value,\n",
    "        use_lightx2v=ui.use_lightx2v.value,\n",
    "        lightx2v_Strength=ui.lightx2v_Strength.value,\n",
    "        lightx2v_steps=ui.steps.value, # steps 값으로 설정\n",
    "        use_pusa=ui.use_lightx2v2.value,\n",
    "        pusa_Strength=ui.lightx2v2_Strength.value,\n",
    "        pusa_steps=ui.steps.value, # steps 값으로 설정\n",
    "        use_sage_attention=ui.use_sage_attention.value,\n",
    "        enable_flow_shift=ui.use_flow_shift.value,\n",
    "        shift=ui.flow_shift.value,\n",
    "        enable_flow_shift2=ui.use_flow_shift.value,\n",
    "        shift2=ui.flow_shift2.value,\n",
    "        end_step1=ui.high_noise_steps.value\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    mins, secs = divmod(duration, 60)\n",
    "    print(f\"Seed: {seed_val}\")\n",
    "    print(f\"✅ Generation completed in {int(mins)} min {secs:.2f} sec\")\n",
    "\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b5144-1d2c-46cc-bb22-ed92d87399c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 셀 5: 프레임 보간 (Frame Interpolation)\n",
    "import glob\n",
    "import time\n",
    "\n",
    "if not ui.interpolate_video.value:\n",
    "    print(\"☑️ Frame interpolation skipped as per UI setting.\")\n",
    "elif not output_path or not os.path.exists(output_path):\n",
    "    print(\"‼️ No video file found from the previous step. Cannot interpolate.\")\n",
    "else:\n",
    "    print(\"✨ Applying Frame Interpolation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # UI에서 값 가져오기\n",
    "    frame_multiplier_val = ui.frame_multiplier.value\n",
    "    interpolated_fps_val = ui.interpolated_fps.value\n",
    "    crf_val = ui.crf_value.value\n",
    "\n",
    "    print(f\"Original video path: {output_path}\")\n",
    "    print(f\"Converting video. Multiplier: {frame_multiplier_val}x, Target FPS: {interpolated_fps_val}, CRF: {crf_val}\")\n",
    "\n",
    "    # os.chdir 대신 %cd 매직 커맨드 사용\n",
    "    %cd /content/Practical-RIFE\n",
    "    \n",
    "    # 추론 스크립트 실행 (os.system 대신 ! 사용)\n",
    "    inference_command = f\"python3 inference_video.py --multi={frame_multiplier_val} --fps={interpolated_fps_val} --video='{output_path}' --scale=1\"\n",
    "    !{inference_command}\n",
    "\n",
    "    # 결과 파일 찾기 및 변환\n",
    "    video_folder = \"/content/ComfyUI/output/\"\n",
    "    # RIFE는 원본 파일명에 _[multiplier]x.mp4 를 추가하여 저장합니다.\n",
    "    base_name = os.path.splitext(os.path.basename(output_path))[0]\n",
    "    interpolated_video_path = os.path.join(video_folder, f\"{base_name}_{frame_multiplier_val}X_30fps.mp4\")\n",
    "\n",
    "    if os.path.exists(interpolated_video_path):\n",
    "        final_output_path = \"/content/Practical-RIFE/output_converted.mp4\"\n",
    "        # ffmpeg 명령어 실행 (os.system 대신 ! 사용)\n",
    "        ffmpeg_command = f\"ffmpeg -i '{interpolated_video_path}' -vcodec libx264 -crf {crf_val} -preset fast '{final_output_path}' -loglevel error -y\"\n",
    "        !{ffmpeg_command}\n",
    "        \n",
    "        print(f\"Displaying final video: {final_output_path}\")\n",
    "        display_video(final_output_path)\n",
    "    else:\n",
    "        print(f\"❌ Interpolated video not found at expected path: {interpolated_video_path}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    mins, secs = divmod(duration, 60)\n",
    "    print(f\"✅ Frame Interpolation completed in {int(mins)} min {secs:.2f} sec\")\n",
    "\n",
    "    clear_memory()\n",
    "    # 작업 후 원래 디렉토리로 돌아감\n",
    "    %cd /content/ComfyUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c45b3-5894-4be2-b2ec-3304981b727b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 셀 6: 파일 삭제\n",
    "!rm -R /content/ComfyUI/input/*\n",
    "!rm -R /content/ComfyUI/output/*\n",
    "print(\"삭제완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051c1ca-08b8-4995-b0a6-ca10aa082599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
